{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "# Make sure server_pull.py file is in the same working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you are connected to NUS wifi or NUS VPN in order to import and run this function\n",
    "# see the link below\n",
    "# https://webvpn.nus.edu.sg/dana-na/auth/url_52/welcome.cgi\n",
    "\n",
    "# Use server_pull_updated\n",
    "#from server_pull_updated import pull\n",
    "from server_pull import pull\n",
    "\n",
    "# pull function has the following syntax: pull(user_group, secret_key, arm)\n",
    "# please change 'test_user' and 'aaaaaaaa' to your own user_group and secret_key respectively\n",
    "\n",
    "output = pull('user32','cnERvnBr',24)\n",
    "#output = pull('user32','cnERvnBr',2)\n",
    "output\n",
    "\n",
    "# output comes as a dictionary with 4 key-value pairs; \n",
    "# Arm you pulled, Net Reward so far, How many times you have pulled, and reward from current pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NetReward = 0\n",
    "NetPull = 0\n",
    "number_machine = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull(user_group, secret_key, machine_num):\n",
    "    global NetReward\n",
    "    global NetPull\n",
    "    \n",
    "    NetReward = NetReward + 1\n",
    "    NetPull = NetPull + 1\n",
    "    \n",
    "    return {'Arm': str(machine_num), 'NetReward': NetReward, 'Pull':NetPull, 'Reward':  np.random.randint(0, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def adaptive_epsilon_greedy(user_group, password, given_rounds, number_machine):\n",
    "    # Initialize the result dict with arms and statistics\n",
    "    result = {idx + 1: {\n",
    "                        \"Arm\": idx + 1,\n",
    "                        \"Output_Arm\": 0,\n",
    "                        \"Total\": 0, \n",
    "                        \"Reward_List\": [], \n",
    "                        \"Total_Reward\": 0,\n",
    "                        } for idx in range(number_machine)\n",
    "             }\n",
    "\n",
    "    # Intial Setting\n",
    "    initial_epsilon = 1.0  # Starting value for epsilon\n",
    "    min_epsilon = 0.01  # Minimum value for epsilon\n",
    "    epsilon_decay_rate = 0.005  # Decay rate for epsilon\n",
    "\n",
    "    epsilon = initial_epsilon\n",
    "    epsilons = []  # List of Chaning Epsilon\n",
    "    \n",
    "    # Learning Process for Exploitation / Exploration\n",
    "    for num_round in range(1, given_rounds + 1):\n",
    "        # Decide whether to explore or exploit\n",
    "        if random.random() < epsilon:\n",
    "            # Explore: randomly choose an arm\n",
    "            arm = random.randint(1, number_machine)\n",
    "            print(f\"# {num_round}, Exploring Arm {arm}...\")\n",
    "        else:\n",
    "            # Exploit: choose the arm with the highest average reward\n",
    "            arm = max(result, key=lambda k: sum(result[k][\"Reward_List\"])/result[k][\"Total\"] if result[k][\"Total\"] > 0 else 0)\n",
    "            print(f\"# {num_round}, Exploiting Arm {arm}...\")\n",
    "        \n",
    "        # Simulate pulling an arm and getting an output\n",
    "        output = pull(user_group, password, arm)\n",
    "        \n",
    "        # Append Data\n",
    "        result[arm][\"Output_Arm\"] = output[\"Arm\"]\n",
    "        result[arm][\"Reward_List\"].append(output[\"Reward\"])\n",
    "        result[arm][\"Total\"] += 1\n",
    "        result[arm][\"Total_Reward\"] = sum(result[arm][\"Reward_List\"])\n",
    "        \n",
    "        # Decay epsilon using time-dependent exponential decay\n",
    "        epsilon = max(min_epsilon, initial_epsilon * math.exp(-epsilon_decay_rate * num_round))\n",
    "        epsilons.append(epsilon)\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'result'는 위에서 얻은 결과 딕셔너리입니다.\n",
    "\n",
    "# Reset the each of arm as List (각 암의 라운드별 누적 보상을 저장할 리스트를 초기화)\n",
    "cumulative_rewards = {arm: [0] for arm in result}\n",
    "\n",
    "# Accumulate the reward (누적 보상)\n",
    "for arm in result:\n",
    "    cumulative = 0\n",
    "    for reward in result[arm][\"Reward_List\"]:\n",
    "        cumulative += reward\n",
    "        cumulative_rewards[arm].append(cumulative)\n",
    "\n",
    "# Ploting Graph (그래프)\n",
    "plt.figure(figsize=(25, 15))\n",
    "for arm in cumulative_rewards:\n",
    "    plt.plot(cumulative_rewards[arm], label=f\"Arm {arm}\")\n",
    "\n",
    "# Add title and label (그래프 제목 및 축 레이블을 추가)\n",
    "plt.title(\"Cumulative Rewards of Each Arm Over Rounds\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
